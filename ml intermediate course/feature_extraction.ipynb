{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ec5f4f",
   "metadata": {},
   "source": [
    "# Word Embbeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262cb272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata-kata yang mirip dengan bakso:  [('seperti', 0.2528931498527527), ('nasi', 0.1701788604259491), ('enak', 0.15016479790210724)]\n",
      "Vektor untuk bakso [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
      "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fepriyadi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_data = {\n",
    "    'Saya suka makan bakso',\n",
    "    'Bakso enak dan lezat',\n",
    "    'Makanan favorit saya adalah nasi goreng',\n",
    "    'Nasi goreng pedas adalah makanan favorit saya',\n",
    "    'Saya suka makanan manis seperti es krim'\n",
    "}\n",
    "\n",
    "tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "word_vectors = model.wv\n",
    "\n",
    "similar_words = word_vectors.most_similar('bakso', topn=3)\n",
    "print(\"Kata-kata yang mirip dengan bakso: \", similar_words)\n",
    "\n",
    "vector = word_vectors['bakso']\n",
    "print(\"Vektor untuk bakso\", vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00483ee1",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f669db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'saya': 14, 'suka': 16, 'makan': 9, 'bakso': 1, 'enak': 3, 'dan': 2, 'lezat': 8, 'makanan': 10, 'favorit': 5, 'adalah': 0, 'nasi': 12, 'goreng': 6, 'pedas': 13, 'manis': 11, 'seperti': 15, 'es': 4, 'krim': 7}\n",
      "TF-IDF Matrix\n",
      "[[0.         0.49851188 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.61789262 0.         0.\n",
      "  0.         0.         0.34810993 0.         0.49851188]\n",
      " [0.         0.42224214 0.52335825 0.52335825 0.         0.\n",
      "  0.         0.         0.52335825 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.43951606 0.         0.         0.         0.         0.43951606\n",
      "  0.43951606 0.         0.         0.         0.36483803 0.\n",
      "  0.43951606 0.         0.30691325 0.         0.        ]\n",
      " [0.38596041 0.         0.         0.         0.         0.38596041\n",
      "  0.38596041 0.         0.         0.         0.320382   0.\n",
      "  0.38596041 0.47838798 0.26951544 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.42966246 0.\n",
      "  0.         0.42966246 0.         0.         0.28774996 0.42966246\n",
      "  0.         0.         0.24206433 0.42966246 0.34664897]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [ \"Saya suka makan bakso\",\n",
    "             \"Bakso enak dan lezat\",\n",
    "             \"Makanan favorit saya adalah nasi goreng\",\n",
    "             \"Nasi goreng pedas adalah makanan favorit saya\",\n",
    "             \"Saya suka makanan manis seperti es krim\"\n",
    "             ]\n",
    "\n",
    "tfid_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfid_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary: \", tfid_vectorizer.vocabulary_)\n",
    "print(\"TF-IDF Matrix\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fe49f",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a13aaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Bow\n",
      "[[1 1 1 1 0 0 1]\n",
      " [1 1 1 1 1 0 0]\n",
      " [1 1 1 1 0 1 0]\n",
      " [1 3 0 1 0 0 0]]\n",
      "\n",
      "Daftar Fitur\n",
      "['adalah' 'contoh' 'dokumen' 'ini' 'kedua' 'ketiga' 'pertama']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"Ini adalah contoh dokumen pertama\",\n",
    "    \"Ini adalah contoh dokumen kedua\",\n",
    "    \"Ini adalah contoh dokumen ketiga\",\n",
    "    \"Ini adalah contoh contoh contoh\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "bow_matrix.toarray()\n",
    "\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"Matrix Bow\")\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "print(\"\\nDaftar Fitur\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e668a1",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2a7422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kalimat:  Saya suka makan bakso enak di warung dekat rumah.\n",
      "1-gram:\n",
      "('Saya',)\n",
      "('suka',)\n",
      "('makan',)\n",
      "('bakso',)\n",
      "('enak',)\n",
      "('di',)\n",
      "('warung',)\n",
      "('dekat',)\n",
      "('rumah.',)\n",
      "2-gram:\n",
      "('Saya', 'suka')\n",
      "('suka', 'makan')\n",
      "('makan', 'bakso')\n",
      "('bakso', 'enak')\n",
      "('enak', 'di')\n",
      "('di', 'warung')\n",
      "('warung', 'dekat')\n",
      "('dekat', 'rumah.')\n",
      "3-gram:\n",
      "('Saya', 'suka', 'makan')\n",
      "('suka', 'makan', 'bakso')\n",
      "('makan', 'bakso', 'enak')\n",
      "('bakso', 'enak', 'di')\n",
      "('enak', 'di', 'warung')\n",
      "('di', 'warung', 'dekat')\n",
      "('warung', 'dekat', 'rumah.')\n",
      "\n",
      "Kalimat:  Nasi goreng adalah salah satu makanan favorit saya.\n",
      "1-gram:\n",
      "('Nasi',)\n",
      "('goreng',)\n",
      "('adalah',)\n",
      "('salah',)\n",
      "('satu',)\n",
      "('makanan',)\n",
      "('favorit',)\n",
      "('saya.',)\n",
      "2-gram:\n",
      "('Nasi', 'goreng')\n",
      "('goreng', 'adalah')\n",
      "('adalah', 'salah')\n",
      "('salah', 'satu')\n",
      "('satu', 'makanan')\n",
      "('makanan', 'favorit')\n",
      "('favorit', 'saya.')\n",
      "3-gram:\n",
      "('Nasi', 'goreng', 'adalah')\n",
      "('goreng', 'adalah', 'salah')\n",
      "('adalah', 'salah', 'satu')\n",
      "('salah', 'satu', 'makanan')\n",
      "('satu', 'makanan', 'favorit')\n",
      "('makanan', 'favorit', 'saya.')\n",
      "\n",
      "Kalimat:  Es krim coklat sangat lezat dan menyegarkan.\n",
      "1-gram:\n",
      "('Es',)\n",
      "('krim',)\n",
      "('coklat',)\n",
      "('sangat',)\n",
      "('lezat',)\n",
      "('dan',)\n",
      "('menyegarkan.',)\n",
      "2-gram:\n",
      "('Es', 'krim')\n",
      "('krim', 'coklat')\n",
      "('coklat', 'sangat')\n",
      "('sangat', 'lezat')\n",
      "('lezat', 'dan')\n",
      "('dan', 'menyegarkan.')\n",
      "3-gram:\n",
      "('Es', 'krim', 'coklat')\n",
      "('krim', 'coklat', 'sangat')\n",
      "('coklat', 'sangat', 'lezat')\n",
      "('sangat', 'lezat', 'dan')\n",
      "('lezat', 'dan', 'menyegarkan.')\n",
      "\n",
      "Kalimat:  Saat hari hujan, saya suka minum teh hangat.\n",
      "1-gram:\n",
      "('Saat',)\n",
      "('hari',)\n",
      "('hujan,',)\n",
      "('saya',)\n",
      "('suka',)\n",
      "('minum',)\n",
      "('teh',)\n",
      "('hangat.',)\n",
      "2-gram:\n",
      "('Saat', 'hari')\n",
      "('hari', 'hujan,')\n",
      "('hujan,', 'saya')\n",
      "('saya', 'suka')\n",
      "('suka', 'minum')\n",
      "('minum', 'teh')\n",
      "('teh', 'hangat.')\n",
      "3-gram:\n",
      "('Saat', 'hari', 'hujan,')\n",
      "('hari', 'hujan,', 'saya')\n",
      "('hujan,', 'saya', 'suka')\n",
      "('saya', 'suka', 'minum')\n",
      "('suka', 'minum', 'teh')\n",
      "('minum', 'teh', 'hangat.')\n",
      "\n",
      "Kalimat:  Pemandangan pegunungan di pagi hari sangat indah.\n",
      "1-gram:\n",
      "('Pemandangan',)\n",
      "('pegunungan',)\n",
      "('di',)\n",
      "('pagi',)\n",
      "('hari',)\n",
      "('sangat',)\n",
      "('indah.',)\n",
      "2-gram:\n",
      "('Pemandangan', 'pegunungan')\n",
      "('pegunungan', 'di')\n",
      "('di', 'pagi')\n",
      "('pagi', 'hari')\n",
      "('hari', 'sangat')\n",
      "('sangat', 'indah.')\n",
      "3-gram:\n",
      "('Pemandangan', 'pegunungan', 'di')\n",
      "('pegunungan', 'di', 'pagi')\n",
      "('di', 'pagi', 'hari')\n",
      "('pagi', 'hari', 'sangat')\n",
      "('hari', 'sangat', 'indah.')\n",
      "\n",
      "Kalimat:  Bola basket adalah olahraga favorit saya sejak kecil.\n",
      "1-gram:\n",
      "('Bola',)\n",
      "('basket',)\n",
      "('adalah',)\n",
      "('olahraga',)\n",
      "('favorit',)\n",
      "('saya',)\n",
      "('sejak',)\n",
      "('kecil.',)\n",
      "2-gram:\n",
      "('Bola', 'basket')\n",
      "('basket', 'adalah')\n",
      "('adalah', 'olahraga')\n",
      "('olahraga', 'favorit')\n",
      "('favorit', 'saya')\n",
      "('saya', 'sejak')\n",
      "('sejak', 'kecil.')\n",
      "3-gram:\n",
      "('Bola', 'basket', 'adalah')\n",
      "('basket', 'adalah', 'olahraga')\n",
      "('adalah', 'olahraga', 'favorit')\n",
      "('olahraga', 'favorit', 'saya')\n",
      "('favorit', 'saya', 'sejak')\n",
      "('saya', 'sejak', 'kecil.')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "sentences = [\n",
    "    \"Saya suka makan bakso enak di warung dekat rumah.\",\n",
    "    \"Nasi goreng adalah salah satu makanan favorit saya.\",\n",
    "    \"Es krim coklat sangat lezat dan menyegarkan.\",\n",
    "    \"Saat hari hujan, saya suka minum teh hangat.\",\n",
    "    \"Pemandangan pegunungan di pagi hari sangat indah.\",\n",
    "    \"Bola basket adalah olahraga favorit saya sejak kecil.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    unigrams = list(ngrams(words, 1))\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "    trigrams = list(ngrams(words, 3))\n",
    "\n",
    "    print(\"\\nKalimat: \", sentence)\n",
    "    print(\"1-gram:\")\n",
    "    for gram in unigrams:\n",
    "        print(gram)\n",
    "    \n",
    "    print(\"2-gram:\")\n",
    "    for gram in bigrams:\n",
    "        print(gram)\n",
    "\n",
    "    print(\"3-gram:\")\n",
    "    for gram in trigrams:\n",
    "        print(gram)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
