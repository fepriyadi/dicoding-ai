{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9812f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli:  Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\n",
      "Tesk setelah filtering:  Perekonomian Indonesia pertumbuhan membanggakan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fepriyadi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/fepriyadi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "##Stopwords NLTK\n",
    "\n",
    "#Download korpus stopwords bahasa indonesia dari NLTK jika belum terunduh\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\"\n",
    "\n",
    "#tokenisasi teks menjadi kata-kata\n",
    "tokens_kata = word_tokenize(teks)\n",
    "\n",
    "#ambil daftar stopwords bahasa indonesia dari NLTK\n",
    "stopwords_indonesia = set(stopwords.words('indonesian'))\n",
    "\n",
    "#filtering kata-kata dengan menghapus stopwords\n",
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]\n",
    "\n",
    "#Gabungkan kata-kata penting kembali menjadi teks\n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    "\n",
    "print(\"Teks asli: \", teks)\n",
    "print(\"Tesk setelah filtering: \", teks_tanpa_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb3446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli :  Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\n",
      "Teks setelah filtering:  Perekonomian Indonesia sedang pertumbuhan membanggakan\n"
     ]
    }
   ],
   "source": [
    "#Stopwords Sastrawi\n",
    "\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Inisialisasi objek StopWordRemover dari Sastrawi\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords_sastrawi = factory.get_stop_words()\n",
    "\n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\"\n",
    "\n",
    "#Tokenisasi teks menjadi kata-kata\n",
    "tokens_kata = word_tokenize(teks)\n",
    "\n",
    "#Filtering kata-kata dengan menghapus stopwords Sastrawi\n",
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_sastrawi]\n",
    "\n",
    "#gabungkan kata kata penting kembali menjadi teks\n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    "\n",
    "print(\"Teks asli : \", teks)\n",
    "print(\"Teks setelah filtering: \", teks_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0ee2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ini', 'adalah', 'contoh', 'tokenisasi', 'kata', 'dalam', 'pemrosesan', 'teks']]\n",
      "\n",
      "[['Ini adalah contoh tokenisasi kalimat.', 'Apakah ini kalimat kedua?', 'Ya, ini kalimat ketiga']]\n",
      "\n",
      "[['Pemrosesan', 'teks', 'adalah', 'cabang', 'ilmu', 'komputer', 'yang', 'berfokus', 'pada', 'pengolahan', 'teks', 'dan', 'dokumen', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer\n",
    "\n",
    "teks = \"Ini adalah contoh tokenisasi kata dalam pemrosesan teks\"\n",
    "tokens = word_tokenize(teks)\n",
    "\n",
    "kalimat = \"Ini adalah contoh tokenisasi kalimat. Apakah ini kalimat kedua? Ya, ini kalimat ketiga\"\n",
    "sentences = sent_tokenize(kalimat)\n",
    "\n",
    "frasa = \"Pemrosesan teks adalah cabang ilmu komputer yang berfokus pada pengolahan teks dan dokumen.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "phrases = tokenizer.tokenize(frasa)\n",
    "\n",
    "print(f\"{[tokens]}\\n\")\n",
    "print(f\"{[sentences]}\\n\")\n",
    "print(f\"{[phrases]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c21a3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yg', 'diperlukan']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Pertama, kita perlu menyiapkan bahan-bahan yg diperlukan\"\n",
    "tokens = re.findall(r'\\w+|\\d+', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f2f801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kata asli: running, kata setelah stemming: run\n",
      "kata asli: runs, kata setelah stemming: run\n",
      "kata asli: runner, kata setelah stemming: runner\n",
      "kata asli: ran, kata setelah stemming: ran\n",
      "kata asli: easily, kata setelah stemming: easili\n",
      "kata asli: fairness, kata setelah stemming: fair\n",
      "kata asli: better, kata setelah stemming: better\n",
      "kata asli: best, kata setelah stemming: best\n",
      "kata asli: cats, kata setelah stemming: cat\n",
      "kata asli: cacti, kata setelah stemming: cacti\n",
      "kata asli: geese, kata setelah stemming: gees\n",
      "kata asli: rocks, kata setelah stemming: rock\n",
      "kata asli: oxen, kata setelah stemming: oxen\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runs\", \"runner\", \"ran\", \"easily\", \"fairness\", \"better\", \"best\", \"cats\", \"cacti\", \"geese\", \"rocks\", \"oxen\"]\n",
    "\n",
    "for word in words:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    print(f\"kata asli: {word}, kata setelah stemming: {stemmed_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5608f83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fepriyadi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bermasalah', 'malemklo', 'sedikitpunnonton', 'sendirimungkin', 'leave']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"bermasalah\", \"malemklo\", \"sedikitpunnonton\", \"sendirimungkin\", \"leaves\"]\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word.lower(), pos=wordnet.VERB) for word in words]\n",
    "\n",
    "print(lemmatized_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
